{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Modeling in Python\n",
    "Masked Language Modeling (MLM) is a key technique used in natural language processing (NLP), particularly in training models like BERT (Bidirectional Encoder Representations from Transformers). The idea is to mask certain words in a sentence and train the model to predict them. This helps the model learn contextual representations of words.\n",
    "### TODO: contextual representations of words.\n",
    "### Step 1: Install Required Libraries\n",
    "First, you need to install the transformers and datasets libraries from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lanita\\Documents\\GitHub\\machine_learning_studies\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Pre-trained BERT Model and Tokenizer\n",
    "Load a pre-trained BERT model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Text for Masking\n",
    "Let's prepare a sample sentence and mask one of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] transformers are [MASK] state - of - the - art technique in natural language processing. [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Transformers are a state-of-the-art technique in natural language processing.\"\n",
    "\n",
    "# Tokenize the text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Create a mask for a particular word\n",
    "masked_index = 3  # Mask the word \"a\"\n",
    "inputs['input_ids'][0][masked_index] = tokenizer.mask_token_id\n",
    "\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perform Masked Language Modeling\n",
    "Use the model to predict the masked word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: a\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "predictions = outputs.logits\n",
    "\n",
    "# Get the predicted token ID for the masked position\n",
    "predicted_token_id = predictions[0, masked_index].argmax(axis=-1).item()\n",
    "\n",
    "# Decode the predicted token ID to get the predicted word\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(f\"Predicted word: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Using a Pipeline for Convenience\n",
    "Hugging Face also provides a pipeline for masked language modeling which makes this process even easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9940366744995117, 'token': 1037, 'token_str': 'a', 'sequence': 'transformers are a state - of - the - art technique in natural language processing.'}, {'score': 0.002596732461825013, 'token': 1996, 'token_str': 'the', 'sequence': 'transformers are the state - of - the - art technique in natural language processing.'}, {'score': 0.0013455171138048172, 'token': 2178, 'token_str': 'another', 'sequence': 'transformers are another state - of - the - art technique in natural language processing.'}, {'score': 0.0012169501278549433, 'token': 2019, 'token_str': 'an', 'sequence': 'transformers are an state - of - the - art technique in natural language processing.'}, {'score': 0.0004081668157596141, 'token': 2028, 'token_str': 'one', 'sequence': 'transformers are one state - of - the - art technique in natural language processing.'}]\n"
     ]
    }
   ],
   "source": [
    "mlm_pipeline = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "result = mlm_pipeline(\"Transformers are [MASK] state-of-the-art technique in natural language processing.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation with Visualization\n",
    "To visualize the process, we can use a simple diagram to explain the flow:\n",
    "\n",
    "Input Sentence: \"Transformers are a state-of-the-art technique in natural language processing.\"\n",
    "Masked Sentence: \"Transformers are [MASK] state-of-the-art technique in natural language processing.\"\n",
    "Tokenization: Convert words into token IDs.\n",
    "Masking: Replace the token ID for the word \"a\" with the mask token ID.\n",
    "Prediction: Use the model to predict the masked token.\n",
    "Decoding: Convert the predicted token ID back to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Create a simple visualization\n",
    "def create_visualization():\n",
    "    width, height = 800, 400\n",
    "    image = Image.new('RGB', (width, height), 'white')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    # Define text and positions\n",
    "    texts = [\n",
    "        \"Input Sentence: Transformers are a state-of-the-art technique in natural language processing.\",\n",
    "        \"Masked Sentence: Transformers are [MASK] state-of-the-art technique in natural language processing.\",\n",
    "        \"Tokenization: ['Transformers', 'are', '[MASK]', 'state', '-', 'of', '-', 'the', '-', 'art', 'technique', 'in', 'natural', 'language', 'processing', '.']\",\n",
    "        \"Prediction: Model predicts the masked token\",\n",
    "        \"Decoding: Predicted word is 'a'\"\n",
    "    ]\n",
    "    positions = [(10, 10), (10, 50), (10, 90), (10, 130), (10, 170)]\n",
    "\n",
    "    # Draw text\n",
    "    for text, position in zip(texts, positions):\n",
    "        draw.text(position, text, fill='black', font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "visualization = create_visualization()\n",
    "visualization.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
